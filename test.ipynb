{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+Ou0YidDwJB2CBEQFjQNB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YFmMAIC_dTsc"},"source":["## Mount google drive and change working directory"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFeLkZT4QqtG","executionInfo":{"status":"ok","timestamp":1617890045203,"user_tz":-540,"elapsed":98682,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"80a39fab-71cf-4065-b039-cf8ab2c4e36b"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = False)\n","\n","%cd /content/drive/My\\ Drive/Colab\\ Notebooks/CS495\\ Individual\\ Study/3회차/RotNet-OOD"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks/CS495 Individual Study/3회차/RotNet-OOD\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e4FwiSPGdaXF"},"source":["## Import libraries and choose device"]},{"cell_type":"code","metadata":{"id":"V5iHb9WPQgqT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617852764169,"user_tz":-540,"elapsed":781,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"4fc9a547-c2c2-480e-b83e-4cc638cfd7f4"},"source":["# import argparse\n","import easydict\n","import random \n","from tqdm import tqdm \n","\n","import torch\n","import torch.nn as nn \n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader as dataloader\n","import torchvision.datasets as datasets\n","\n","if torch.cuda.is_available():\n","    torch.cuda.set_device(0)\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","print(device)\n","\n","from sklearn.metrics import roc_auc_score\n","\n","from models.allconv import AllConvNet\n","from models.wrn_prime import WideResNet\n","from RotDataset import RotDataset\n","from utils import * "],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kcW2oXbadj5l"},"source":["## Define argparser"]},{"cell_type":"code","metadata":{"id":"D6ScBE4_QlJC"},"source":["def arg_parser():\n","    parser = argparse.ArgumentParser('argument for training')\n","    parser.add_argument('--seed', type=int, default=0)\n","\n","    parser.add_argument('--method', type=str, default='rot', help='rot, msp')\n","    parser.add_argument('--ood_dataset', type=str, default='cifar100', help='cifar100 | svhn')\n","    parser.add_argument('--num_workers', type=int, default=8)\n","\n","    # Optimization options\n","    parser.add_argument('--epochs', '-e', type=int, default=100, help='Number of epochs to train.')\n","    parser.add_argument('--learning_rate', '-lr', type=float, default=0.1, help='The initial learning rate.')\n","    parser.add_argument('--batch_size', '-b', type=int, default=128, help='Batch size.')\n","    parser.add_argument('--test_bs', type=int, default=200)\n","    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum.')\n","    parser.add_argument('--decay', '-d', type=float, default=0.0005, help='Weight decay (L2 penalty).')\n","    parser.add_argument('--rot-loss-weight', type=float, default=0.5, help='Multiplicative factor on the rot losses')\n","\n","    # WRN Architecture\n","    parser.add_argument('--layers', default=40, type=int, help='total number of layers')\n","    parser.add_argument('--widen-factor', default=2, type=int, help='widen factor')\n","    parser.add_argument('--droprate', default=0.3, type=float, help='dropout probability')\n","\n","    args = parser.parse_args()\n","\n","    return args\n","\n","def easy_dict():\n","    args = easydict.EasyDict({\n","        \"seed\": 0,\n","\n","        \"method\": 'rot',\n","        'ood_dataset': 'cifar100',\n","        'num_workers': 8,\n","\n","        'epochs': 100,\n","        'learning_rate': 0.1,\n","        'batch_size': 128,\n","        'test_bs': 200,\n","        'momentum': 0.9,\n","        'decay': 0.0005,\n","        'rot-loss-weight': 0.5,\n","\n","        'layers': 40,\n","        'widen_factor': 2,\n","        'droprate': 0.3\n","    })\n","\n","    return args"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eEsF4KLLdpOz"},"source":["## Define main function"]},{"cell_type":"code","metadata":{"id":"fRT6b65cPkDD"},"source":["def main():\n","    # arg parser\n","    # args = arg_parser()\n","    args = easy_dict()\n","\n","    # set seed\n","    set_seed(args.seed)  \n","    \n","    # dataset \n","    id_testdata = datasets.CIFAR10('./data/', train=False, download=True)\n","    id_testdata = RotDataset(id_testdata, train_mode=False)\n","\n","    if args.ood_dataset == 'cifar100':\n","        ood_testdata = datasets.CIFAR100('./data/', train=False, download=True)\n","    elif args.ood_dataset == 'svhn':\n","        ood_testdata = datasets.SVHN('./data/', split='test', download=True)\n","    else:\n","        raise ValueError(args.ood_dataset)\n","    ood_testdata = RotDataset(ood_testdata, train_mode=False)\n","    \n","    # data loader  \n","    id_test_loader = dataloader(id_testdata, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=True)\n","    ood_test_loader = dataloader(ood_testdata, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=True)\n","  \n","    # load model\n","    num_classes = 10\n","    model = WideResNet(args.layers, num_classes, args.widen_factor, dropRate=args.droprate)\n","    model.rot_head = nn.Linear(128, 4)\n","    model = model.to(device)\n","    model.load_state_dict(torch.load('./models/trained_model_{}.pth'.format(args.method), map_location = device))\n","\n","    # TODO:\n","    ## 1. calculate ood score by two methods(MSP, Rot)\n","    id_testdata_score, ood_testdata_score = [], []\n","\n","    for x_tf_0, x_tf_90, x_tf_180, x_tf_270, batch_y in tqdm(id_test_loader):\n","        batch_size = x_tf_0.shape[0]\n","        batch_x = torch.cat([x_tf_0, x_tf_90, x_tf_180, x_tf_270], 0).to(device)\n","        batch_y = batch_y.to(device)\n","        batch_rot_y = torch.cat((\n","            torch.zeros(batch_size),\n","            torch.ones(batch_size),\n","            2 * torch.ones(batch_size),\n","            3 * torch.ones(batch_size)\n","        ), 0).long().to(device)\n","        \n","        logits, pen = model(batch_x)\n","\n","        classification_logits = logits[:batch_size]\n","        rot_logits = model.rot_head(pen)\n","\n","        classification_loss = torch.max(classification_logits, dim = -1)[0].data\n","        lotation_loss = F.cross_entropy(rot_logits, batch_rot_y, reduce = False).data\n","\n","        uniform_distribution = torch.zeros_like(classification_logits).fill_(1 / num_classes)\n","        kl_divergence_loss = F.kl_div(input = classification_logits, target = uniform_distribution, reduce = False).data\n","\n","        for i in range(batch_size):\n","            msp_score = - classification_loss[i]\n","            rot_score = - torch.sum(kl_divergence_loss[i]) + 1 / 4 * (lotation_loss[i] + lotation_loss[i + batch_size] + lotation_loss[i + 2 * batch_size] + lotation_loss[i + 3 * batch_size])\n","            if args.method == 'rot':\n","                score = rot_score\n","            elif args.method == 'msp':\n","                score = msp_score\n","\n","            id_testdata_score.append(score)\n","\n","    for x_tf_0, x_tf_90, x_tf_180, x_tf_270, batch_y in tqdm(ood_test_loader):\n","        batch_size = x_tf_0.shape[0]\n","        batch_x = torch.cat([x_tf_0, x_tf_90, x_tf_180, x_tf_270], 0).to(device)\n","        batch_y = batch_y.to(device)\n","        batch_rot_y = torch.cat((\n","            torch.zeros(batch_size),\n","            torch.ones(batch_size),\n","            2 * torch.ones(batch_size),\n","            3 * torch.ones(batch_size)\n","        ), 0).long().to(device)\n","        \n","        logits, pen = model(batch_x)\n","\n","        classification_logits = logits[:batch_size]\n","        rot_logits = model.rot_head(pen)\n","\n","        classification_loss = torch.max(classification_logits, dim = -1)[0].data\n","        lotation_loss = F.cross_entropy(rot_logits, batch_rot_y, reduce = False).data\n","\n","        uniform_distribution = torch.zeros_like(classification_logits).fill_(1 / num_classes)\n","        kl_divergence_loss = F.kl_div(input = classification_logits, target = uniform_distribution, reduce = False).data\n","\n","        for i in range(batch_size):\n","            msp_score = - classification_loss[i]\n","            rot_score = - torch.sum(kl_divergence_loss[i]) + 1 / 4 * (lotation_loss[i] + lotation_loss[i + batch_size] + lotation_loss[i + 2 * batch_size] + lotation_loss[i + 3 * batch_size])\n","            if args.method == 'rot':\n","                score = rot_score\n","            elif args.method == 'msp':\n","                score = msp_score\n","\n","            ood_testdata_score.append(score)\n","\n","    y_true = torch.cat((\n","        torch.zeros(len(id_testdata_score)),\n","        torch.ones(len(ood_testdata_score))\n","    ), 0)\n","\n","    y_score = torch.cat((\n","        torch.tensor(id_testdata_score),\n","        torch.tensor(ood_testdata_score)\n","    ), 0).long()\n","\n","    ## 2. calculate AUROC by using ood scores\n","    print(roc_auc_score(y_true, y_score, average = None))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uWz1LEyldr6n"},"source":["## Call main function"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sxhjm6mSQpBL","executionInfo":{"status":"ok","timestamp":1617853129011,"user_tz":-540,"elapsed":19688,"user":{"displayName":"김창훈","photoUrl":"","userId":"04041019206976312811"}},"outputId":"8643310e-9115-4feb-a89c-ff2d20ba08ab"},"source":["main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","  0%|          | 0/79 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","100%|██████████| 79/79 [00:08<00:00,  9.66it/s]\n","100%|██████████| 79/79 [00:08<00:00,  9.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["0.6731077300000001\n"],"name":"stdout"}]}]}